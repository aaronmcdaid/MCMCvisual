---
title: "*REGEL - AB Testing with the Ratio of the Expected Gain to the Expected Loss*"
output: html_document
params:
    publication_date: "2nd January 2019"
---

```{r child='../_header_and_footer/header.Rmd'}
```

```{r setup, echo=FALSE}
library(knitr)
knitr::opts_chunk$set(engine.path = list(python = '/usr/bin/python3.6'))
```
```{python, echo=F}
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
```

\newcommand{\HatDelta}{\hat\Delta}
\newcommand{\VarHatDelta}{\operatorname{Var}[\HatDelta]}

# &nbsp;


In this post I propose a simple method for AB Testing, using Bayesian ideas.
But my evaluation is frequentist, as I discuss the Type I error rate.
And even though there are a lot of complex simulations in this post, the method itself is quite simple and efficient -
it uses only the information that is required to perform a t-test.

I think it's a new method, but please tell me if you recognize anything!

(There is a [library associated with this post on Github](https://github.com/aaronmcdaid/bayesian.risk), where I also include the Jupyter notebook, on )

The basic idea is to answer these two questions:

 - **Stopping Criterion**: When do we stop the experiment?
 - **Decision Criterion**: And, once stopped, how do we choose between these three possible decisions?
    1. _Positive_ - variant B is better.
    1. _Neutral_ - unclear which variant is better.
    1. _Negative_ - variant A is better.

If the effect is strong, we want the experiment to stop early.

## Student's t-test

To begin, let's recap the [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_unequal_sample_sizes,_equal_variance).
We have a sample from each of the two group, with estimates of the mean in each group, $\hat\mu_A$ and $\hat\mu_B$.
We also have [estimates](https://en.wikipedia.org/wiki/Bessel%27s_correction) of the variance in each group, $\hat\sigma^2_A$ and $\hat\sigma^2_B$.

The _pooled variance estimate_ is a weighted combination of the two variance estimates:

$$ \hat\sigma_{pooled} = \frac{(n_A-1)\hat\sigma^2_A + (n_B-1)\hat\sigma^2_B}{n_A+n_B-2} $$
The variance of the estimator of the mean tells us how close the estimated mean is to the true mean:

$$ \mathrm{Var}(\hat\mu_A) = \frac1{n_A} \hat\sigma_{pooled} $$

$$ \mathrm{Var}(\hat\mu_B) = \frac1{n_B} \hat\sigma_{pooled} $$

The variance of a sum (or difference) is the sum of the variances.
Therefore, if we estimate the difference in the means as

$$ \HatDelta = (\hat\mu_B - \hat\mu_A) $$

then the variance of the estimator $\HatDelta$ is
$$
\begin{aligned}
    \VarHatDelta
    & = \mathrm{Var}(\hat\mu_B - \hat\mu_A) \\
    & = \mathrm{Var}(\hat\mu_A) + \mathrm{Var}(\hat\mu_B) \\
    & = \frac1{n_A}\hat\sigma^2_{pooled}
     +\frac1{n_B}\hat\sigma^2_{pooled}
\end{aligned}
$$

Now, we have the estimate of the effect, $\HatDelta$, and the variance of
that estimate, $\VarHatDelta$, and we could proceed to compute a p-value.
However, in this post I'll take a different approach.

## Normality, and the posterior

I'm going to assume the likelihood function is [normal](https://en.wikipedia.org/wiki/Gaussian_distribution).
This will be an important assumption later in this post when I discuss sufficient statistics.
But it really just means that I'm assuming the sample size is reasonably large.
If you want to use just a few dozen samples, then this method isn't for you!

I'm just using a flat prior here. The prior doesn't matter if you have a large sample size.
I _do_ find other priors really interesting, and I really enjoyed writing
[this blog post](../unit.information.priors/)
about _unit information priors_!
But it doesn't matter for large sample sizes.

Therefore, the posterior distribution used in this method is a normal distribution:

$$ \Delta|\cdot \sim \mathcal{N}(\HatDelta, \VarHatDelta) $$

## Expected Loss and Expected Gain

The posterior is interpreted as an estimate of the effect, and the uncertainty of that effect.
Imagine drawing numbers randomly from the posterior, then replacing the positive numbers with zero,
and then compute the mean of this distribution.
This is the [_expected loss_](https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5) of variant B.

$$ EL = \mathbb{E}\mathopen{}\left[\operatorname{min}\left(0, \mathcal{N}\mathopen{}\left(\HatDelta, \VarHatDelta\right)\mathclose{}\right)\right]\mathclose{} $$

In the [library associated with this post](https://github.com/aaronmcdaid/bayesian.risk/blob/master/bayesianAB/risk.py)
includes python code for computing this, named [`slow_risk`](https://github.com/aaronmcdaid/bayesian.risk/blob/master/bayesianAB/risk.py#L18).
In that library, I use the term _risk_ as shorthand for _expected loss_.

I define _expected gain_ as the opposite, i.e. the result when replacing negative values with zero.

$$ EG = \mathbb{E}\mathopen{}\left[\operatorname{max}\left(0, \mathcal{N}\mathopen{}\left(\HatDelta, \VarHatDelta\right)\mathclose{}\right)\right]\mathclose{} $$

## Why compute the EL and EG?

As we run the experiment, we compute both EL and EG as often as we like. Perhaps after every data point, or perhaps every hour.
If there truly is no difference between the performance of A and the performance of B,
then both EL and EG will gradually approach zero.
They will never reach exactly zero. In fact, EL will always be negative, and EG will always be positive.
But they will each become arbitrarily to zero if you keeping running the experiment.
For the remainder of this post, I will usually use $(-EL)$ in the formulae as it
will always be positive and is easier to reason about.

If B is better, i.e. the true effect $\Delta_0 > 0$, then EL will tend to zero and EG will tend to $\Delta_0$.
Formally, we can say that as the number of samples $n$ tends to infinity,

$$
    \lim_{n\rightarrow\infty} EL = \min(0, \Delta_0) \\
$$
$$
    \lim_{n\rightarrow\infty} EG = \max(0, \Delta_0) \\
$$

We can see this in the following figure, showing how EL and EG behave as the experiment proceeds
under three different values of the true effect $\Delta_0 = 0.2$, $\Delta_0 = 0$, $\Delta_0 = 0.5$.
Each plot includes 50 simulated experiments:

<center>
![_Figure 1_:
Trend of EL and EG as samples are gathered.
50 simulations were run under each of three conditions.
](figures/tendency_for_ELEG.png)

&nbsp;
</center>

## When to stop?

One simple approach, discussed [here](https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5),
is to end the experiment when either EL or EG become sufficiently close to zero,
as defined by a user-defined _threshold-of-caring_, $\tau$.

$$\min(-EL, EG) < \tau$$

While it certainly is a simple approach, it has the disadvantage that the _runtime_,
i.e.  the number of samples needed to reach this stopping condition,
has a huge variance.
Sometimes it can take a very long time to reach this stopping condition - and it's likely
that you will be tempted to simply end the experiment early in that case.
We see this in the next figure, where 1000 simulations were performed under the null hypothesis
and then the runtimes were sorted and plotted in blue.
I normalized the data such that the average runtime is 5,000 samples.
As you can see in the top right of _Figure 2_, this simple policy took 60,000 samples to reach this
simple stopping condition under one of the simulations.
For comparison, two other stopping conditions are included.

<center>
![_Figure 2_:
Three different stopping criteria are shown here.
1000 simulations under the null are run until the criterion is reached,
and then the runtimes (numbers of samples) are sorted and plotted.
](figures/three_runtimes.png)

&nbsp;
</center>

The second approach uses the product, instead of the minimum, of the EL and the EG and stops
the experiment when the product, $(-EL) \times EG$ comes below a predefined threshold.
As can be seen above, the maximum runtime is much smaller.

The method I present in this paper, which I call _REGEL_ (Ratio of Expected Gain and Expected Loss),
uses the product of three factors. This is the _REGEL_ stopping condition:
$$ (-EL) \times (EG{+}(-EL)) \times EG \qquad < \qquad ??? $$

When this falls below a predefined threshold (I define the threshold later in this post), the experiment is stopped.
As you can see in the plot, the maximum runtime under REGEL is only a little larger than the mean.
None of the simulations run for much longer than the mean runtime.

Of course, if our goal is to simply minimize the maximum runtime, we could just insert a hard upper
bound on the sample size. However, for a variety of reasons, I want a stopping
criterion and decision criterion based strictly on EL and EG and on nothing else.

## Decision time: Negative, Neutral, or Positive?

Once the stopping condition has been reached and the experiment has been stopped, we still
have to decide on the outcome. Which variant, if any, is better?
This is done using the ratio of EG to the EL. If the ratio is very large or very small,
we have a Positive or Negative decision. Otherwise, we have a Neutral decision.
For this blog post, I use a ratio of 350. In other words, after the experiment has stopped:

 - **Positive** , i.e. variant B is better, if $EG > 350 \times (-EL)$
 - **Negative** , i.e. variant A is better, if $EG < \frac1{350} \times (-EL)$
 - **Neutral** , otherwise

```{python, dpi=50, fig.width=6, fig.height=6, fig.align='center', echo=F}
from bayesianAB.risk import slow_risk
from scipy.optimize import newton

# define the stopping criterion
def _REGEL_stopping_criterion(EG, EL):
    return EG*EL*(EG+EL)

# we stop when this threshold is reached:
threshold = 1

# For each possible EL, use Newton-Raphson to find the EG at
# which the stopping criterion is first reached.

ELs = np.linspace(0.0001, 100, 100000)
EGs = np.array([
        newton( # Newton-Raphson
            lambda EG: _REGEL_stopping_criterion(EG, EL) - threshold
        , 1 # initial guess for Newton-Raphson
        ) for EL in ELs])

# Plot the 'surface' along which the stopping is first reached
plt.loglog(ELs, EGs, label='Neutral')
plt.xlabel("""EL""")
plt.ylabel('EG')

plt.text(0.1, 0.2,'$stopped$')
plt.text(1, 2,'$not$ $yet$ $stopped$')

ratio = 350
positive_decisions = EGs/ELs > ratio
negative_decisions = EGs/ELs < 1/ratio
plt.plot(ELs[positive_decisions], EGs[positive_decisions], color='green', linewidth=5, label='Positive')
plt.plot(ELs[negative_decisions], EGs[negative_decisions], color='red', linewidth=5, label='Negative')
plt.legend()
plt.show()
```
<center>
_Figure 3_: The curve is the range of (EL,EG) pairs for which the stopping criterion is reached.
The colour tells us which decision is taken.

&nbsp;
</center>

## Why 350?

You might ask why I use a ratio of 350 in the decision criterion.
That's the first "magic number" in this blog posts.
The reason is that it gives a
[Type I error rate](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#Type_I_error)
of 10%.
In other words, if the null hypothesis is true
(i.e. there is truly no difference between between the two variants)
and we run an experiment until the REGEL stopping criterion is reached,
approximately 10% of the simulation will result in a non-Neutral decision.
5% will be Positive and 5% will be Negative.

I discovered this via simulations. Bear in mind that other stopping criteria will result in a different relationship
between the decision ratio and the Type I error rate.
You might be skeptical and wonder how transferable this is. Different simulations with different parameters
might lead to different results? Actually no, this ratio always gives a Type I error rate of 10% when
combined with the REGEL stopping criterion, not matter what threshold is used within the stopping criterion (see Figure 4).
The only requirement is that the threshold be small enough to require at least a few hundred samples.

<center>
![_Figure 4_:
{_Left panel_}: No matter what threshold is used for the _REGEL stopping condition_, the
distribution of the final ratio EG/EL is the same.
The x-axis is the density, across 1000 simulations, of the value $\log(EG/EL)$
when the stopping criterion is satisfied. Each colour respresents a different
threshold. \n
{_right panel_}: On the other hand, the sample sizes are affected by the threshold.
A smaller threshold, as expected, leads to longer running experiments.
](figures/350_is_10percent.png)

&nbsp;

</center>

I also experimented with many other simulations to verify the robustness of this.
For example, I considered a 10%/90% split between the two variants instead of the usual 50%/50% split.
But to really prove that this is a general result, some mathematical proof is required.
So I'll attempt to do so here

(Next couple of paragraphs are quite technical. Although, I think I would like to make them even
more technical! Maybe you can help me to firm this up.)

As described at the start of this post, we assume the likelihood and the posterior
are normally distributed.
This is reasonable given a large enough sample size, and
also given that the model is about the difference of two sample means.
In other words, the
[Central Limit Theorem](https://en.wikipedia.org/wiki/Central_Limit_Theorem) applies.

This assumption allows us to treat the mean and variance as [_sufficient statistics_](https://en.wikipedia.org/wiki/sufficient_statistic)
to describe the distribution.
The EL and EG are functions of the mean and variance. Also, we can compute the mean and variance
from the EL and EG.
Therefore, we can say that the EL and EG are sufficient statistics for the posterior distribution.

This is useful because it means that two different experiments that have the same EL and also
have the same EG can be said to be in the same "state", and the future behaviour of the EL and EG (as new data is observed)
is the same in both.



_... maths stuff, like sufficient statistics and scale-invariance and so on ..._





&nbsp;

Thanks for making it this far! I'd love feedback, see my Twitter handle and my email address at the top of the page.

```{r child='../_header_and_footer/footer.Rmd'}
```
