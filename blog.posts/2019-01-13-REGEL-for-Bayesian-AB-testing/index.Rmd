---
title: "*REGEL - AB Testing with the Ratio of the Expected Gain to the Expected Loss*"
output: html_document
params:
    publication_date: "2nd January 2019"
---

```{r child='../_header_and_footer/header.Rmd'}
```

```{r setup, echo=FALSE}
library(knitr)
knitr::opts_chunk$set(engine.path = list(python = '/usr/bin/python3.6'))
```

\newcommand{\HatDelta}{\hat\Delta}
\newcommand{\VarHatDelta}{\operatorname{Var}[\HatDelta]}

# &nbsp;


In this post I propose a simple method for AB Testing, using Bayesian ideas.
But my evaluation is frequentist, as I discuss the Type I error rate.
And even though there are a lot of complex simulations in this post, the method itself is quite simple and efficient -
it uses only the information that is required to perform a t-test.

I think it's a new method, but please tell me if you recognize anything!

The basic idea is to answer these two questions:

 - **Stopping Criterion**: When do we stop the experiment?
 - **Decision Criterion**: And, once stopped, how do we choose between these three possible decisions?
    1. _Positive_ - variant B is better.
    1. _Neutral_ - unclear which variant is better.
    1. _Negative_ - variant A is better.

If the effect is strong, we want the experiment to stop early.

## Student's t-test

To begin, let's recap the [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Equal_or_unequal_sample_sizes,_equal_variance).
We have a sample from each of the two group, with estimates of the mean in each group, $\hat\mu_A$ and $\hat\mu_B$.
We also have [estimates](https://en.wikipedia.org/wiki/Bessel%27s_correction) of the variance in each group, $\hat\sigma^2_A$ and $\hat\sigma^2_B$.

The _pooled variance estimate_ is a weighted combination of the two variance estimates:

$$ \hat\sigma_{pooled} = \frac{(n_A-1)\hat\sigma^2_A + (n_B-1)\hat\sigma^2_B}{n_A+n_B-2} $$
The variance of the estimator of the mean tells us how close the estimated mean is to the true mean:

$$ \mathrm{Var}(\hat\mu_A) = \frac1{n_A} \hat\sigma_{pooled} $$

$$ \mathrm{Var}(\hat\mu_B) = \frac1{n_B} \hat\sigma_{pooled} $$

The variance of a sum (or difference) is the sum of the variances.
Therefore, if we estimate the difference in the means as

$$ \HatDelta = (\hat\mu_B - \hat\mu_A) $$

then the variance of the estimator $\HatDelta$ is
$$
\begin{aligned}
    \VarHatDelta
    & = \mathrm{Var}(\hat\mu_B - \hat\mu_A) \\
    & = \mathrm{Var}(\hat\mu_A) + \mathrm{Var}(\hat\mu_B) \\
    & = \frac1{n_A}\hat\sigma^2_{pooled}
     +\frac1{n_B}\hat\sigma^2_{pooled}
\end{aligned}
$$

Now, we have the estimate of the effect, $\HatDelta$, and the variance of
that estimate, $\VarHatDelta$, and we could proceed to compute a p-value.
However, in this post I'll take a different approach.

## Normality, and the posterior

I'm going to assume the likelihood function is [normal](https://en.wikipedia.org/wiki/Gaussian_distribution).
This will be an important assumption later in this post when I discuss sufficient statistics.
But it really just means that I'm assuming the sample size is reasonably large.
If you want to use just a few dozen samples, then this method isn't for you!

I'm just using a flat prior here. The prior doesn't matter if you have a large sample size.
I _do_ find other priors really interesting, and I really enjoyed writing
[this blog post](../unit.information.priors/)
about _unit information priors_!
But it doesn't matter for large sample sizes.

Therefore, the posterior distribution used in this method is a normal distribution:

$$ \Delta|\cdot \sim \mathcal{N}(\HatDelta, \VarHatDelta) $$

## Expected Loss and Expected Gain

The posterior is interpreted as an estimate of the effect, and the uncertainty of that effect.
Imagine drawing numbers randomly from the posterior, then replacing the positive numbers with zero,
and then compute the mean of this distribution.
This is the [_expected loss_](https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5) of variant B.

$$ EL = \mathbb{E}\mathopen{}\left[\operatorname{min}\left(0, \mathcal{N}\mathopen{}\left(\HatDelta, \VarHatDelta\right)\mathclose{}\right)\right]\mathclose{} $$

In the [library associated with this post](https://github.com/aaronmcdaid/bayesian.risk/blob/master/bayesianAB/risk.py)
includes python code for computing this, named [`slow_risk`](https://github.com/aaronmcdaid/bayesian.risk/blob/master/bayesianAB/risk.py#L18).
In that library, I use the term _risk_ as shorthand for _expected loss_.

I define _expected gain_ as the opposite, i.e. the result when replacing negative values with zero.

$$ EG = \mathbb{E}\mathopen{}\left[\operatorname{max}\left(0, \mathcal{N}\mathopen{}\left(\HatDelta, \VarHatDelta\right)\mathclose{}\right)\right]\mathclose{} $$

## Why compute the EL and EG?

As we run the experiment, we compute both EL and EG as often as we like. Perhaps after every data point, or perhaps every hour.
If there truly is no difference between the performance of A and the performance of B, then both EL and EG will eventually
reach zero.
They will never reach exactly zero. In fact, EL will always be negative, and EG will always be positive.
But they will each become arbitrarily to zero if you keeping running the experiment.

If B is better, i.e. the true effect $\Delta_0 > 0$, then EL will tend to zero and EG will tend to $\Delta_0$.
Formally, we can say that as the number of samples $n$ tends to infinity,

$$
    \lim_{n\rightarrow\infty} EL = \min(0, \Delta_0) \\
$$
$$
    \lim_{n\rightarrow\infty} EG = \max(0, \Delta_0) \\
$$

We can see this in the following figure, showing how EL and EG behave as the experiment proceeds
under three different values of the true effect $\Delta_0 = 0.2$, $\Delta_0 = 0$, $\Delta_0 = 0.5$.
Each plot includes 50 simulated experiments:

![](figures/tendency_for_ELEG.png)

## When to stop?

One simple approach, discussed [here](https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5),
is to end the experiment when either EL or EG become sufficiently close to zero,
as defined by a user-defined _threshold-of-caring_, $\tau$.

$$\min(-EL, EG) < \tau$$

While it certainly is a simple approach, it has the disadvantage that the _runtime_,
i.e.  the number of samples needed to reach this stopping condition,
has a huge variance.
Sometimes it can take a very long time to reach this stopping condition - and it's likely
that you will be tempted to simply end the experiment early in that case.
We see this in the next figure, where 1000 simulations were performed under the null hypothesis
and then the runtimes were sorted and plotted in blue.
I normalized the data such that the average runtime is 5,000 samples.
As you can see in the top right, this simple policy took 60,000 samples to reach this
simple stopping condition under one of the simulations.
For comparison, two other stopping conditions are included.

![](figures/three_runtimes.png)

The second approach uses the product, instead of the minimum, of the EL and the EG and stops
the experiment when the product, $(-EL) \times EG$ comes below a predefined threshold.
As can be seen above, the maximum runtime is much smaller.

The method I present in this paper, which I call _REGEL_ (Ratio of Expected Gain and Expected Loss),
uses the product of three factors. This is the _REGEL_ stopping condition:
$$ (-EL) \times (EG{+}(-EL)) \times EG \qquad < \qquad ??? $$

When this falls below a predefined threshold (more on this below..), the experiment is stopped.
As you can see in the plot, the maximum runtime under REGEL is only a little larger than the mean.
None of the simulations run for much longer than the mean runtime.

Of course, if our goal is to simply minimize the maximum runtime, we could just insert a hard upper
bound on the sample size. However, for a variety of reasons, I am keen to have a stopping
criterion and decision criterion based strictly on EL and EG and on nothing else.


&nbsp;

Thanks for making it this far! I'd love feedback, see my Twitter handle and my email address at the top of the page.

```{r child='../_header_and_footer/footer.Rmd'}
```
